\relax 
\providecommand\hyper@newdestlabel[2]{}
\@nameuse{bbl@beforestart}
\catcode `"\active 
\catcode `<\active 
\catcode `>\active 
\@nameuse{es@quoting}
\AC@reset@newl@bel
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\providecommand \oddpage@label [2]{}
\babel@aux{spanish}{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Ejercicio sobre la búsqueda iterativa de óptimos}{3}{section.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Implementación del algoritmo de gradiente descendente}{3}{subsection.1.1}\protected@file@percent }
\@writefile{loa}{\contentsline {algocf}{\numberline {1}{\ignorespaces Gradient Descent\relax }}{3}{algocf.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Ejecución del algoritmo de gradiente descendente para la función $E$}{3}{subsection.1.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Gráfica de $E(u,v)$ junto con el mínimo encontrado por gradiente descendente\relax }}{4}{figure.caption.2}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:ej1.2}{{1}{4}{Gráfica de $E(u,v)$ junto con el mínimo encontrado por gradiente descendente\relax }{figure.caption.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}Ejecución del algoritmo de gradiente descendente para la función $f$}{4}{subsection.1.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Gráfica de $f(x,y)$ junto con el mínimo encontrado por gradiente descendente\relax }}{5}{figure.caption.3}\protected@file@percent }
\newlabel{fig:ej1.3_graficas}{{2}{5}{Gráfica de $f(x,y)$ junto con el mínimo encontrado por gradiente descendente\relax }{figure.caption.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Evolución de los valores devueltos por los mínimos obtenidos en cada iteración\relax }}{5}{figure.caption.4}\protected@file@percent }
\newlabel{fig:ej1.3_evolucion}{{3}{5}{Evolución de los valores devueltos por los mínimos obtenidos en cada iteración\relax }{figure.caption.4}{}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Tabla con los mínimos obtenidos para cada punto inicial\relax }}{6}{table.caption.5}\protected@file@percent }
\newlabel{fig:ej1.3_puntos_iniciales}{{1}{6}{Tabla con los mínimos obtenidos para cada punto inicial\relax }{table.caption.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4}Conclusión sobre la dificultad de encontrar el mínimo global de una función arbitraria}{6}{subsection.1.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Ejercicio sobre regresión lineal}{7}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Ajuste de modelos de regresión lineal a vectores de características extraídos de imágenes de dígitos manuscritos}{7}{subsection.2.1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Tabla con los errores de la función de pérdida y de clasificación obtenido en SGD y en pseudoinversa\relax }}{8}{table.caption.7}\protected@file@percent }
\newlabel{fig:ej2.1_errores}{{2}{8}{Tabla con los errores de la función de pérdida y de clasificación obtenido en SGD y en pseudoinversa\relax }{table.caption.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Soluciones obtenidas en ambos algoritmos junto con los conjuntos de datos\relax }}{8}{figure.caption.6}\protected@file@percent }
\newlabel{fig:ej2.1_soluciones}{{4}{8}{Soluciones obtenidas en ambos algoritmos junto con los conjuntos de datos\relax }{figure.caption.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Ajuste de modelos de regresión lineal con aumento de complejidad}{8}{subsection.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.1}Experimento con transformaciones lineales}{8}{subsubsection.2.2.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Muestra de entrenamiento de $N = 1000$ en el cuadrado $X = [-1,1] \times [-1,1]$\relax }}{9}{figure.caption.8}\protected@file@percent }
\newlabel{fig:ej2.2_muestra}{{5}{9}{Muestra de entrenamiento de $N = 1000$ en el cuadrado $X = [-1,1] \times [-1,1]$\relax }{figure.caption.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Muestra de entrenamiento etiquetada con un 10 \% de ruido\relax }}{9}{figure.caption.9}\protected@file@percent }
\newlabel{fig:ej2.2_muestra_etiquetada}{{6}{9}{Muestra de entrenamiento etiquetada con un 10 \% de ruido\relax }{figure.caption.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Recta obtenida por SGD sobre la muestra etiquetada\relax }}{10}{figure.caption.10}\protected@file@percent }
\newlabel{fig:ej2.2_sgd_lineal}{{7}{10}{Recta obtenida por SGD sobre la muestra etiquetada\relax }{figure.caption.10}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.2}Experimento con transformaciones no lineales}{10}{subsubsection.2.2.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Función no lineal obtenida por SGD sobre la muestra etiquetada\relax }}{10}{figure.caption.11}\protected@file@percent }
\newlabel{fig:ej2.2_sgd_no_lineal}{{8}{10}{Función no lineal obtenida por SGD sobre la muestra etiquetada\relax }{figure.caption.11}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Bonus: Método de Newton}{12}{section.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Gráfica de $f(x,y)$ junto con el mínimo encontrado por el método de Newton\relax }}{12}{figure.caption.12}\protected@file@percent }
\newlabel{fig:bonus_graficas}{{9}{12}{Gráfica de $f(x,y)$ junto con el mínimo encontrado por el método de Newton\relax }{figure.caption.12}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Evolución de los valores devueltos por los mínimos obtenidos en cada iteración\relax }}{13}{figure.caption.13}\protected@file@percent }
\newlabel{fig:bonus_evolucion}{{10}{13}{Evolución de los valores devueltos por los mínimos obtenidos en cada iteración\relax }{figure.caption.13}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Tabla con los mínimos obtenidos para cada punto inicial\relax }}{13}{table.caption.14}\protected@file@percent }
\newlabel{fig:bonus_puntos_iniciales}{{3}{13}{Tabla con los mínimos obtenidos para cada punto inicial\relax }{table.caption.14}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Evolución de los valores devueltos por los mínimos obtenidos en cada iteración\relax }}{14}{figure.caption.15}\protected@file@percent }
\newlabel{fig:bonus_evolucion_E}{{11}{14}{Evolución de los valores devueltos por los mínimos obtenidos en cada iteración\relax }{figure.caption.15}{}}
