\babel@toc {spanish}{}
\contentsline {section}{\numberline {1}Ejercicio sobre la búsqueda iterativa de óptimos}{3}{section.1}%
\contentsline {subsection}{\numberline {1.1}Implementación del algoritmo de gradiente descendente}{3}{subsection.1.1}%
\contentsline {subsection}{\numberline {1.2}Ejecución del algoritmo de gradiente descendente para la función $E$}{3}{subsection.1.2}%
\contentsline {subsection}{\numberline {1.3}Ejecución del algoritmo de gradiente descendente para la función $f$}{4}{subsection.1.3}%
\contentsline {subsection}{\numberline {1.4}Conclusión sobre la dificultad de encontrar el mínimo global de una función arbitraria}{6}{subsection.1.4}%
\contentsline {section}{\numberline {2}Ejercicio sobre regresión lineal}{7}{section.2}%
\contentsline {subsection}{\numberline {2.1}Ajuste de modelos de regresión lineal a vectores de características extraídos de imágenes de dígitos manuscritos}{7}{subsection.2.1}%
\contentsline {subsection}{\numberline {2.2}Ajuste de modelos de regresión lineal con aumento de complejidad}{8}{subsection.2.2}%
\contentsline {subsubsection}{\numberline {2.2.1}Experimento con transformaciones lineales}{8}{subsubsection.2.2.1}%
\contentsline {subsubsection}{\numberline {2.2.2}Experimento con transformaciones no lineales}{10}{subsubsection.2.2.2}%
\contentsline {section}{\numberline {3}Bonus: Método de Newton}{12}{section.3}%
